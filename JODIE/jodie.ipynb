{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library_data import *\n",
    "import library_models as lib\n",
    "from library_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(standard_tensor, self_tensor):\n",
    "    cos = nn.CosineSimilarity()\n",
    "    loss = 0\n",
    "    for i, st in enumerate(standard_tensor):\n",
    "        denominator = molecular = 0\n",
    "        for j, se in enumerate(self_tensor):\n",
    "            if i == j:\n",
    "                molecular = torch.exp(cos(st.reshape(1, -1), se.reshape(1, -1)) / 0.07)\n",
    "                continue\n",
    "            denominator += torch.exp(cos(st.reshape(1, -1), se.reshape(1, -1)) / 0.07)\n",
    "        loss += torch.log(molecular / denominator)[0]\n",
    "    return -(1 / len(standard_tensor)) * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE PARAMETERS\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.network = \"lastfm\"\n",
    "        self.train_proportion = 0.8\n",
    "        self.datapath = \"data/%s.csv\" % self.network\n",
    "        self.model = \"jodie\"\n",
    "        self.embedding_dim = 128\n",
    "        self.epochs = 50\n",
    "        self.state_change = False\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tbatch_id</th>\n",
       "      <th>interactionids</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>state_label</th>\n",
       "      <th>user_timediffs</th>\n",
       "      <th>item_timediffs</th>\n",
       "      <th>previous_items</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.046651</td>\n",
       "      <td>-0.109694</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.046624</td>\n",
       "      <td>-0.109637</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>479.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.046407</td>\n",
       "      <td>-0.109192</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3714.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.044754</td>\n",
       "      <td>-0.105804</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7417.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.042863</td>\n",
       "      <td>-0.101926</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293098</th>\n",
       "      <td>139995</td>\n",
       "      <td>1293098</td>\n",
       "      <td>678</td>\n",
       "      <td>870</td>\n",
       "      <td>137097345.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045784</td>\n",
       "      <td>0.228608</td>\n",
       "      <td>644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293099</th>\n",
       "      <td>139996</td>\n",
       "      <td>1293099</td>\n",
       "      <td>678</td>\n",
       "      <td>125</td>\n",
       "      <td>137099570.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045515</td>\n",
       "      <td>0.462956</td>\n",
       "      <td>870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293100</th>\n",
       "      <td>139997</td>\n",
       "      <td>1293100</td>\n",
       "      <td>678</td>\n",
       "      <td>138</td>\n",
       "      <td>137103981.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.044398</td>\n",
       "      <td>0.971983</td>\n",
       "      <td>125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293101</th>\n",
       "      <td>139998</td>\n",
       "      <td>1293101</td>\n",
       "      <td>678</td>\n",
       "      <td>697</td>\n",
       "      <td>137104686.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.046291</td>\n",
       "      <td>4.147203</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293102</th>\n",
       "      <td>139999</td>\n",
       "      <td>1293102</td>\n",
       "      <td>678</td>\n",
       "      <td>662</td>\n",
       "      <td>137107267.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045333</td>\n",
       "      <td>0.434330</td>\n",
       "      <td>697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1293103 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tbatch_id  interactionids  user_id  item_id    timestamp  \\\n",
       "0                1               0        0        0          0.0   \n",
       "1                1               1        1        1         54.0   \n",
       "2                1               3        2        3        479.0   \n",
       "3                1               6        3        6       3714.0   \n",
       "4                1              10        4       10       7417.0   \n",
       "...            ...             ...      ...      ...          ...   \n",
       "1293098     139995         1293098      678      870  137097345.0   \n",
       "1293099     139996         1293099      678      125  137099570.0   \n",
       "1293100     139997         1293100      678      138  137103981.0   \n",
       "1293101     139998         1293101      678      697  137104686.0   \n",
       "1293102     139999         1293102      678      662  137107267.0   \n",
       "\n",
       "         state_label  user_timediffs  item_timediffs  previous_items    0    1  \n",
       "0                  0       -0.046651       -0.109694            1000  0.0  0.0  \n",
       "1                  0       -0.046624       -0.109637            1000  0.0  0.0  \n",
       "2                  0       -0.046407       -0.109192            1000  0.0  0.0  \n",
       "3                  0       -0.044754       -0.105804            1000  0.0  0.0  \n",
       "4                  0       -0.042863       -0.101926            1000  0.0  0.0  \n",
       "...              ...             ...             ...             ...  ...  ...  \n",
       "1293098            0       -0.045784        0.228608             644  0.0  0.0  \n",
       "1293099            0       -0.045515        0.462956             870  0.0  0.0  \n",
       "1293100            0       -0.044398        0.971983             125  0.0  0.0  \n",
       "1293101            0       -0.046291        4.147203             138  0.0  0.0  \n",
       "1293102            0       -0.045333        0.434330             697  0.0  0.0  \n",
       "\n",
       "[1293103 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbatch_table = pd.read_csv(\"results/batches_lastfm.txt\", header=0)\n",
    "tbatch_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interactions = len(tbatch_table[\"interactionids\"])\n",
    "num_users = len(set(tbatch_table[\"user_id\"])) \n",
    "num_items = len(set(tbatch_table[\"item_id\"])) + 1 # one extra item for \"none-of-these\"\n",
    "num_features = len(tbatch_table.iloc[0]) - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET TRAINING, VALIDATION, TESTING, and TBATCH BOUNDARIES\n",
    "train_end_idx = validation_start_idx = int(num_interactions * args.train_proportion) \n",
    "test_start_idx = int(num_interactions * (args.train_proportion+0.1))\n",
    "test_end_idx = int(num_interactions * (args.train_proportion+0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan = tbatch_table[\"timestamp\"].iloc[-1] - tbatch_table[\"timestamp\"][0]\n",
    "tbatch_timespan = timespan / 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Initializing the JODIE model ***\n",
      "Initializing user and item embeddings\n",
      "Initializing user and item RNNs\n",
      "Initializing linear layers\n",
      "*** JODIE initialization complete ***\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE MODEL AND PARAMETERS\n",
    "model = JODIE(args, num_features, num_users, num_items).cuda()\n",
    "crossEntropyLoss = nn.CrossEntropyLoss()\n",
    "MSELoss = nn.MSELoss()\n",
    "binaryCrossEntropyLoss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE EMBEDDING\n",
    "initial_user_embedding = nn.Parameter(F.normalize(torch.rand(args.embedding_dim).cuda(), dim=0)) # the initial user and item embeddings are learned during training as well\n",
    "initial_item_embedding = nn.Parameter(F.normalize(torch.rand(args.embedding_dim).cuda(), dim=0))\n",
    "model.initial_user_embedding = initial_user_embedding\n",
    "model.initial_item_embedding = initial_item_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = initial_user_embedding.repeat(num_users, 1) # initialize all users to the same embedding \n",
    "item_embeddings = initial_item_embedding.repeat(num_items, 1) # initialize all items to the same embedding\n",
    "item_embedding_static = Variable(torch.eye(num_items).cuda()) # one-hot vectors for static embeddings\n",
    "user_embedding_static = Variable(torch.eye(num_users).cuda()) # one-hot vectors for static embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_user_embeddings = initial_user_embedding.repeat(num_users, 1) # initialize all users to the same embedding \n",
    "self_item_embeddings = initial_item_embedding.repeat(num_items, 1) # initialize all items to the same embedding\n",
    "self_item_embedding_static = Variable(torch.eye(num_items).cuda()) # one-hot vectors for static embeddings\n",
    "self_user_embedding_static = Variable(torch.eye(num_users).cuda()) # one-hot vectors for static embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training the JODIE model for 50 epochs ***\n"
     ]
    }
   ],
   "source": [
    "# RUN THE JODIE MODEL\n",
    "'''\n",
    "THE MODEL IS TRAINED FOR SEVERAL EPOCHS. IN EACH EPOCH, JODIES USES THE TRAINING SET OF INTERACTIONS TO UPDATE ITS PARAMETERS.\n",
    "'''\n",
    "print(\"*** Training the JODIE model for %d epochs ***\" % args.epochs)\n",
    "\n",
    "# variables to help using tbatch cache between epochs\n",
    "is_first_epoch = True\n",
    "cached_tbatches_user = {}\n",
    "cached_tbatches_item = {}\n",
    "cached_tbatches_interactionids = {}\n",
    "cached_tbatches_feature = {}\n",
    "cached_tbatches_user_timediffs = {}\n",
    "cached_tbatches_item_timediffs = {}\n",
    "cached_tbatches_previous_item = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e131c61fe8d5497b94df0e8267a0eb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc056370355b4599a368bab6afce0935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=103448.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7733, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.8088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.3861, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(-inf, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f038821b8386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# BACKPROPAGATE ERROR AFTER END OF T-BATCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with trange(args.epochs) as progress_bar1:\n",
    "    for ep in progress_bar1:\n",
    "        progress_bar1.set_description('Epoch %d of %d' % (ep, args.epochs))\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        # INITIALIZE EMBEDDING TRAJECTORY STORAGE\n",
    "        user_embeddings_timeseries = Variable(torch.Tensor(num_interactions, args.embedding_dim).cuda())\n",
    "        item_embeddings_timeseries = Variable(torch.Tensor(num_interactions, args.embedding_dim).cuda())\n",
    "        self_user_embeddings_timeseries = Variable(torch.Tensor(num_interactions, args.embedding_dim).cuda())\n",
    "        self_item_embeddings_timeseries = Variable(torch.Tensor(num_interactions, args.embedding_dim).cuda())\n",
    "        project_timediff = torch.Tensor([5]).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reinitialize_tbatches()\n",
    "        total_loss, loss, total_interaction_count, self_total_interaction_count = 0, 0, 0, 0\n",
    "\n",
    "        tbatch_start_time = None\n",
    "        tbatch_to_insert = -1\n",
    "        tbatch_full = False\n",
    "\n",
    "        # TRAIN TILL THE END OF TRAINING INTERACTION IDX\n",
    "        with trange(train_end_idx) as progress_bar2:\n",
    "            for j in progress_bar2:\n",
    "                progress_bar2.set_description('Processed %dth interactions' % j) \n",
    "                current_batch_data = tbatch_table[tbatch_table[\"tbatch_id\"] == j+1]\n",
    "                drop_indices = np.random.choice(current_batch_data.index, int(len(current_batch_data)*0.3), replace=False)\n",
    "                if len(current_batch_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                batch_user = list(set(current_batch_data[\"user_id\"]))\n",
    "                batch_item = list(set(current_batch_data[\"item_id\"]))\n",
    "                \n",
    "                user_embeddings_projections = Variable(torch.zeros(len(batch_user), args.embedding_dim).cuda())\n",
    "                item_embeddings_projections = Variable(torch.zeros(len(batch_item), args.embedding_dim).cuda())\n",
    "                self_user_embeddings_projections = Variable(torch.zeros(len(batch_user), args.embedding_dim).cuda())\n",
    "                self_item_embeddings_projections = Variable(torch.zeros(len(batch_item), args.embedding_dim).cuda())\n",
    "\n",
    "                # Standard Forward Propagations\n",
    "                for i in range(len(current_batch_data)):\n",
    "                    total_interaction_count += len(current_batch_data)\n",
    "\n",
    "                    tbatch_userids = torch.LongTensor(current_batch_data[\"user_id\"].values[i:i+1]).cuda() # Recall \"lib.current_tbatches_user[i]\" has unique elements\n",
    "                    tbatch_itemids = torch.LongTensor(current_batch_data[\"item_id\"].values[i:i+1]).cuda() # Recall \"lib.current_tbatches_item[i]\" has unique elements\n",
    "                    tbatch_interactionids = torch.LongTensor(current_batch_data[\"interactionids\"].values[i:i+1]).cuda()\n",
    "                    feature_tensor = Variable(torch.Tensor(tbatch_table[tbatch_table.columns[9:]].values[i:i+1]).cuda()) # Recall \"lib.current_tbatches_feature[i]\" is list of list, so \"feature_tensor\" is a 2-d tensor\n",
    "                    user_timediffs_tensor = Variable(torch.Tensor(current_batch_data[\"user_timediffs\"].values[i:i+1]).cuda().unsqueeze(1))\n",
    "                    item_timediffs_tensor = Variable(torch.Tensor(current_batch_data[\"item_timediffs\"].values[i:i+1]).cuda().unsqueeze(1))\n",
    "                    tbatch_itemids_previous = torch.LongTensor(current_batch_data[\"previous_items\"].values[i:i+1]).cuda()\n",
    "                    item_embedding_previous = item_embeddings[tbatch_itemids_previous,:]\n",
    "\n",
    "                    user_embedding_input = user_embeddings[tbatch_userids,:]\n",
    "                    item_embedding_input = item_embeddings[tbatch_itemids,:]\n",
    "                    \n",
    "                    # UPDATE DYNAMIC EMBEDDINGS AFTER INTERACTION\n",
    "                    user_embedding_output = model.forward(user_embedding_input, item_embedding_input, timediffs=user_timediffs_tensor, features=feature_tensor, select='user_update')\n",
    "                    item_embedding_output = model.forward(user_embedding_input, item_embedding_input, timediffs=item_timediffs_tensor, features=feature_tensor, select='item_update')\n",
    "\n",
    "                    item_embeddings[tbatch_itemids,:] = item_embedding_output\n",
    "                    user_embeddings[tbatch_userids,:] = user_embedding_output  \n",
    " \n",
    "                    user_embeddings_timeseries[tbatch_interactionids,:] = user_embedding_output\n",
    "                    item_embeddings_timeseries[tbatch_interactionids,:] = item_embedding_output\n",
    "                    \n",
    "                    # CALCULATE LOSS TO MAINTAIN TEMPORAL SMOOTHNESS\n",
    "                    loss += MSELoss(item_embedding_output, item_embedding_input.detach())\n",
    "                    loss += MSELoss(user_embedding_output, user_embedding_input.detach())\n",
    "                    \n",
    "                    # Self-supervised Forward Propagations\n",
    "                    if i not in drop_indices:\n",
    "                        user_embedding_output = model.forward(user_embedding_input, item_embedding_input, timediffs=user_timediffs_tensor, features=feature_tensor, select='user_update')\n",
    "                        item_embedding_output = model.forward(user_embedding_input, item_embedding_input, timediffs=item_timediffs_tensor, features=feature_tensor, select='item_update')\n",
    "\n",
    "                        self_item_embeddings[tbatch_itemids,:] = item_embedding_output\n",
    "                        self_user_embeddings[tbatch_userids,:] = user_embedding_output  \n",
    "\n",
    "                        self_user_embeddings_timeseries[tbatch_interactionids,:] = user_embedding_output\n",
    "                        self_item_embeddings_timeseries[tbatch_interactionids,:] = item_embedding_output\n",
    "\n",
    "                        # CALCULATE LOSS TO MAINTAIN TEMPORAL SMOOTHNESS\n",
    "                        loss += MSELoss(item_embedding_output, item_embedding_input.detach())\n",
    "                        loss += MSELoss(user_embedding_output, user_embedding_input.detach())\n",
    "                \n",
    "                # PROJECT USER EMBEDDING TO CURRENT TIME\n",
    "                for k in range(len(batch_user)):\n",
    "                    user_embeddings_projections[k] = model.forward(user_embeddings[batch_user[k],:], None, timediffs=project_timediff, features=None, select='project')\n",
    "                    self_user_embeddings_projections[k] = model.forward(self_user_embeddings[batch_user[k],:], None, timediffs=project_timediff, features=None, select='project')\n",
    "                    \n",
    "                # PROJECT ITEM EMBEDDING TO CURRENT TIME\n",
    "                for k in range(len(batch_item)):\n",
    "                    item_embeddings_projections[k] = model.forward(item_embeddings[batch_item[k],:], None, timediffs=project_timediff, features=None, select='project')\n",
    "                    self_item_embeddings_projections[k] = model.forward(self_item_embeddings[batch_item[k],:], None, timediffs=project_timediff, features=None, select='project')\n",
    "\n",
    "                loss += softmax_cross_entropy(user_embeddings_projections, self_user_embeddings_projections.detach_())\n",
    "                loss += softmax_cross_entropy(item_embeddings_projections, self_item_embeddings_projections.detach_())\n",
    "                \n",
    "                # BACKPROPAGATE ERROR AFTER END OF T-BATCH\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # RESET LOSS FOR NEXT T-BATCH\n",
    "                loss = 0\n",
    "                item_embeddings.detach_() # Detachment is needed to prevent double propagation of gradient\n",
    "                user_embeddings.detach_()\n",
    "                item_embeddings_timeseries.detach_() \n",
    "                user_embeddings_timeseries.detach_()\n",
    "                self_item_embeddings.detach_()\n",
    "                self_user_embeddings.detach_()\n",
    "                self_item_embeddings_timeseries.detach_() \n",
    "                self_user_embeddings_timeseries.detach_()\n",
    "\n",
    "        is_first_epoch = False # as first epoch ends here\n",
    "        print(\"Last epoch took {} minutes\".format((time.time()-epoch_start_time)/60))\n",
    "        # END OF ONE EPOCH \n",
    "        print(\"\\n\\nTotal loss in this epoch = %f\" % (total_loss))\n",
    "        item_embeddings_dystat = torch.cat([item_embeddings, item_embedding_static], dim=1)\n",
    "        user_embeddings_dystat = torch.cat([user_embeddings, user_embedding_static], dim=1)\n",
    "        # SAVE CURRENT MODEL TO DISK TO BE USED IN EVALUATION.\n",
    "        save_model(model, optimizer, args, ep, user_embeddings_dystat, item_embeddings_dystat, train_end_idx, user_embeddings_timeseries, item_embeddings_timeseries)\n",
    "\n",
    "        user_embeddings = initial_user_embedding.repeat(num_users, 1)\n",
    "        item_embeddings = initial_item_embedding.repeat(num_items, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END OF ALL EPOCHS. SAVE FINAL MODEL DISK TO BE USED IN EVALUATION.\n",
    "print(\"\\n\\n*** Training complete. Saving final model. ***\\n\\n\")\n",
    "save_model(model, optimizer, args, ep, user_embeddings_dystat, item_embeddings_dystat, train_end_idx, user_embeddings_timeseries, item_embeddings_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tbatch_id,user_id,item_id,timestamp,state_label,comma_separated_list_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[user2id, user_sequence_id, user_timediffs_sequence, user_previous_itemid_sequence,\n",
    " item2id, item_sequence_id, item_timediffs_sequence, \n",
    " timestamp_sequence, feature_sequence, y_true] = load_network(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
